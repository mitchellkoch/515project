\documentclass{article}
\usepackage{amsmath, amssymb, graphicx, subfigure, algorithmic, color}

\author{Mitchell Koch, Justin Huang}
\title{CSE 515 Literature Survey}
\date{Friday, May 17, 2013}

\begin{document}
\maketitle

For our project on structured learning of relation association graphs we chose to focus our literature survey on previous relation entailment work~\cite{Berant:2012:LER:2122944.2122947}, on work in relation extraction using matrix factorization~\cite{riedel13relation}, as well as on related work in probabilistic modeling of relations between entities~\cite{TaskarWAK03}.

\section*{Berant, et al. 2012}

\section*{Riedel, et al. 2013}

This paper focuses on the problem of aggregate relation extraction: extracting relation triples consisting of two arguments and a relation type. This paper unifies doing relation extraction with a fixed schema with an approach which does not use a fixed schema, like Open IE~\cite{Etzioni:2008:OIE:1409360.1409378}. They describe this as a ``universal schema'', the unon of both the surface form predicates as in Open IE and relations in fixed knowledge bases like Freebase. They use a matrix factorization approach similar to collaborative filtering, where relations are like products and tuples for those relations are like customers. A big strength of this approach is that can take advantage of these different kinds of relations and learn jointly from them.

They set up their model to estimate the probability of a tuple being in a relation using log linear models over parameters defined using a latent feature model with latent representations for relaitions and tuples like in generalized PCA. They use a neighborhood approach to interpolate the confidence based on similar relations by using weights for the strengths between relations. They also use an entity model to allow representation of latent types, allowing for representing the compatibility between argument slots and entities.

They use ranked pairs to avoid the need for negative examples using the concept of implicit feedback from collaborative filtering. They maximize an objective with the goal being pairs that are in the observed data rank higher than those not in the observed data and maximize it using Stochastic Gradient Descent.

In their experiments, they use articles from different time ranges in the New York Times for training and testing and also split Freebase facts to align with different facts in the train and test sets. For evaluation they compare their system against several other systems and use an information retrieval-styled evaluation where for each relation, each system gets to choose the tuples it is most confident in and a common pool is chosen from those to evaluate all the sytems. A weakness of this evaluation is that it casts the problem of relation extraction as a problem of information retrieval instead of measuring how many facts in a document are actually extracted correctly. This still should allow relative comparisons, however, and they do outperform the other relation extraction methods. Their combined model including the latent entity representation performs the best for predicting Freebase relations, although it appears better to not use the latent entity representation for surface patterns.

One source of background for this work comes relational clustering, with the goal being to cluster surface patterns to find latent relations; however, in this paper the goal is to predict patterns where they are not observed instead of directly identifying new patterns. Instead of using an assumption of symmetry in clustered patterns, they assume relations entail others.  

We intend to try a similar approach for relation entailment by using a similar matrix of entity pairs and relation types from both Open IE, Freebase, as well as those extracted from Freebase using distant supervision as in~\cite{HoffmannZLZW11}. Some weaknesses of this approach hinge on the fact that it only provides aggregate relation extraction over a whole corpus as opposed to relation extraction for individual sentences; however, for relation entailment this should be less of an issue as the idea is to find in general what relations associate with each other.

\section*{Taskar, et al. 2003}

This paper approaches the problem of predicting links and their types for entities involved in relations. It takes the relational Markov network (RMN) framework of~\cite{Taskar:2002:DPM:2073876.2073934} and extends it to predict not only labels but also links. This is applied to a graph of university webpages as well as a social network. The background is that of Probabilistic Relational Models, a directed analog to RMNs.

For these problems, relational schemas are defined with object types and attributes. In this case, both pages and links are objects, and attributes can be information such as from a bag of words. A RMN is defined to provide cliques and potentials between attributes of related entities using templates, such as between class labels of linked pages. The parameters are learned from data over a single relational instantiation using belief propagation. 

For the university webpages dataset, evidence for relations includes hyperlinks from an entity page or an owned page to another entity's page, as well as virtual links based on mentioning a page title or an email address. They tried observing page categories and not observing them. Using both modeling of website sections along with triad transitivity templates outperformed a flat model evaluating individual links in the observed case. For the unobserved entity label case, they used a similarity model as well as combining with a flat model, which together did bettr than just the flat model but not in all cases. The social network dataset includes information about students and lists of friends. They tried predicting where some links are obseerved in the test data and use cliques modeling shared characteristics between students. Here this similarity model outperforms the flat model as well.

They found that both similarity templates, including links sharing a common endpoint, as well as transitivity templates helped. The strength of this method is clear over flat classification of links, because it allows taking into account more information, and it also performs significantly better than using generative PRMs, because the RMNs are discriminative models. This also allows using subgraph patterns not possible to represent in the directed PRMs. A weakness of this approach is that becaue of the complexity of the problem, belief propagation is used and does not always converge.

We could model the entities and relations that we are dealing with in a similar way. Instead of pages we would have Freebase entities, instead of links we would have relations or surface patterns, and potentially we could have entity types, either from Freebase or latent for labels. The cliques could group together entities appearing in the same sentence or document. Then the relation extraction problem would be to learn more links given some observed links, and the relation entailment problem could be approached by extracting the probabilities of one kind of link given another and forming a corresponding relation entailment graph that agrees. 

\bibliographystyle{abbrv}
\bibliography{bib}

\end{document}
